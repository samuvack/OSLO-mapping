{"0": {
    "doc": "Home",
    "title": "VSDS Technical Documentation",
    "content": " ",
    "url": "/#vsds-technical-documentation",
    
    "relUrl": "/#vsds-technical-documentation"
  },"1": {
    "doc": "Home",
    "title": "Introduction",
    "content": "The Opensource building blocks Linked Data Interactions project is an effort to make interactions with Linked Data more fluently by providing easy building blocks. This project was created in function of the VSDS project . ",
    "url": "/#introduction",
    
    "relUrl": "/#introduction"
  },"2": {
    "doc": "Home",
    "title": "Supported Frameworks",
    "content": "Currently, we support 2 frameworks to use these building blocks in: . | Linked Data Interactions Orchestrator: A lightweight application maintained by the LDI team. | Apache Nifi: A powerful system to easily process and distribute data | . Component support over frameworks . As the LDI team is rather small and focused on supporting the VSDS project, we sometimes have to postpone full integration of our building blocks in all supported frameworks. However, since the LDI project is open source, feel free to contribute and/or create issues at our GitHub project . ",
    "url": "/#supported-frameworks",
    
    "relUrl": "/#supported-frameworks"
  },"3": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"4": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": "Today, the main task of data publishers is to meet the user’s needs and expectations, and they realise this by creating and maintaining multiple querying APIs for their datasets. However, this approach causes various drawbacks for the data publisher. First, keeping multiple APIs online can be costly as the load generated by data consumers is often at the expense of the data publisher. And second, data publishers must ensure their APIs are always up-to-date with the latest standards and technologies, which results in a huge mandatory maintenance effort. As new trends emerge, old APIs may become obsolete and incompatible, creating legacy issues for data consumers who may have to switch to new APIs or deal with outdated data formats and functionalities. Moreover, the existing APIs limit data reuse and innovation since the capabilities and limitations of these APIs constrain data consumers. They can only create their views or indexes on top of the data using a technology they prefer. “Linked Data Event Streams as the base API to publish datasets” . On the other hand, data consumers often have to deal with multiple versions or copies of a dataset that need to be more consistent or synchronised. These versions or copies may have different snapshots or deltas published at other times or frequencies. Although using data dumps provides a data consumer with complex flexibility regarding the needed functionality, this approach also has various drawbacks. For example, data consumers must track when and how each version or copy was created and updated. They also have to compare different versions or copies to identify changes or conflicts in the data. Data consumers may need to be more consistent due to outdated or incomplete versions or copies. They may also miss significant changes or updates in data not reflected in their versions or copies. To overcome these challenges, Linked Data Event Streams (LDES) provide a generic and flexible base API for datasets. With LDES, data consumers can set up workflow to automatically replicate the history of a dataset and stay in sync with the latest updates. ",
    "url": "/introduction/Introduction",
    
    "relUrl": "/introduction/Introduction"
  },"5": {
    "doc": "LDES Client",
    "title": "LDES Client",
    "content": ". The LDES CLIENT is designed for replication and synchronisation, meaning the client can retrieve members of an LDES but also checks regularly if new members are added and fetch them, allowing data consumers to stay up to date with the dataset. To understand the functioning of an LDES client, it is important to understand how LDESes are published on the Web. The Linked Data Fragments principle is utilised for publishing an LDES, meaning that the data is published in one or more fragments and meaningful semantic links are created between these fragments. This approach facilitates clients to follow these links and discover additional data. However, the critical aspect for the LDES client is the notion of mutable and immutable fragments. When publishing an LDES stream, a common configuration is to have a maximum number of members per fragment. Once a fragment surpasses this limit, it is regarded as immutable, and a ‘Cache-control: immutable’ cache header is added to the fragment to signify this. This information is crucial for the LDES client since it only needs to retrieve an immutable fragment once, while mutable fragments must be regularly polled to identify new members. ",
    "url": "/introduction/LDES_client",
    
    "relUrl": "/introduction/LDES_client"
  },"6": {
    "doc": "LDES Client",
    "title": "Replication",
    "content": "To initiate the replication of an LDES, data consumers must configure the LDES client with an LDES endpoint. If multiple views are available for the LDES, the LDES client will begin the replication process utilising the first view it receives. However, suppose a data consumer has a specific preference for a particular view to initiate the replication. In that case, it is also possible to configure the view URI in the LDES client accordingly. When the client visits a fragment, it parses the content to RDF and discovers LDES members by looking for triples with the tree:member. Moreover, the client searches for triples with a tree:relation predicate, signifying links to other fragments, and adds them to its queue of fragments to be fetched if they still need to be retrieved. The client inspects the response headers for each fragment and looks for a potential ‘Cache-control: immutable’ attribute, indicating that the fragment is immutable and does not need to be polled again. ",
    "url": "/introduction/LDES_client#replication",
    
    "relUrl": "/introduction/LDES_client#replication"
  },"7": {
    "doc": "LDES Client",
    "title": "Synchronisation",
    "content": "In addition to replication, the LDES client keeps track of all mutable fragments and periodically polls them to check if new LDES members were added. To further optimise the synchronisation process, the LDES client reads the ‘Cache-control: max-age’ value from the response headers, which specifies the time period for which the LDES fragment remains valid. This allows the LDES client to schedule periodic polling more efficiently. By utilising the response headers provided by the LDES server, the LDES client can effectively manage the synchronisation of the LDES stream, while minimising the amount of processing required. This makes the LDES client an efficient and reliable tool for processing and managing Linked Data Event Streams. ",
    "url": "/introduction/LDES_client#synchronisation",
    
    "relUrl": "/introduction/LDES_client#synchronisation"
  },"8": {
    "doc": "LDES Client",
    "title": "Resuming",
    "content": "An essential functionality of the LDES client is resuming, enabling the client to halt and resume from where it last stopped. To achieve this functionality, the client utilises an SQLite database to persist the immutable and mutable fragment IDs, guaranteeing that an immutable fragment is only retrieved once. The member IDs are also saved in the database for mutable fragments, ensuring that an LDES member is processed only once. ",
    "url": "/introduction/LDES_client#resuming",
    
    "relUrl": "/introduction/LDES_client#resuming"
  },"9": {
    "doc": "LDES Client",
    "title": "Linked Data Interactions",
    "content": "The LDES client component is written in Java and available as an SDK in the Linked Data Interactions repository. More information about Linked Data Interactions can be found here. ",
    "url": "/introduction/LDES_client#linked-data-interactions",
    
    "relUrl": "/introduction/LDES_client#linked-data-interactions"
  },"10": {
    "doc": "LDES Server",
    "title": "LDES Server",
    "content": "The Linked Data Event Stream (LDES) server is a configurable component that can be used to ingest, store, and (re-)publish one or multiple Linked Data Event Stream(s). The open-source LDES server is built in the context of the VSDS project to exchange (Open) Data easily. The server can be configured to meet the organisation’s specific needs. Functionalities include retention policy, fragmentation, deletion, create a snapshot and pagination for managing and processing large amounts of data more efficiently and ensuring the efficient use of storage. The LDES server is available as on open-source building block on [GitHub](https://github.com/Informatievlaanderen/VSDS-LDESServer4J) . ",
    "url": "/introduction/LDES_server",
    
    "relUrl": "/introduction/LDES_server"
  },"11": {
    "doc": "LDES Server",
    "title": "Setting up the LDES Server during startup process",
    "content": "Ingesting sources (HTTP in) . The LDES server is able to receive data via HTTP ingestion. Specifically, the server expects a single object (member) to be sent as input via a POST request. If the dataset still contains state objects, each of these must first be converted to a version object before being ingested in the server. This essential step ensures the ingested objects comply with the LDES definition. Once the objects in the dataset are LDES-compliant members (whether or not after conversion to a version object) and the LDES member has been added to the LDES server, the server can effortlessly publish the LDES member as part of the LDES. More information on the HTTP ingestion can be found here. Example HTTP Ingest-Fetch Configuration: . server.port: { http-port } ldes: collection-name: { short name of the collection, cannot contain characters that are used for url interpretation, e.g.’ $’, ‘=’ or ‘&amp;’, } host-name: { hostname of LDES Server } member-type: { Defines which syntax type is used to define the member id e.g. “https://data.vlaanderen.be/ns/mobiliteit#Mobiliteitshinder”, } timestamp-path: { SHACL property path to the timestamp when the version object entered the event stream., } version-of: { SHACL property path to the non-versioned identifier of the entity. } validation: shape: { URI to defined shape } enabled: { Enables/Disables shacl validation on ingested members } rest: max-age: { time in seconds that a mutable fragment can be considered up-to-date, default when omitted: 60, } max-age-immutable: { time in seconds that an immutable fragment should not be refreshed, default when omitted: 604800, } . SHACL . The LDES specification prescribes that each LDES must link to a SHACL shape, providing a machine-readable definition of the members in the collection. If a SHACL shape was provided on startup, the LDES server reads it before the ingestion process starts and the SHACL shape is used to validate the ingested members. Only valid members are ingested in the LDES server. When starting the server, it is possible to provide a SHACL shape via through an RDF file. At last, the SHACL shape is also published as part of the LDES on the Web. SHACL stands for Shapes Constraint Language and is used to define a set of constraints which are used to verify to conformity of RDF data with these constraints. The SHACL shape specifies the expected properties of an LDES members and the constraints that must be followed to ensure the LDES member adheres to the expected structure and semantics. It defines properties such as required properties, allowed property values, and the data types expected for the properties. For more information about the SHACL shape and its structure, go to here. More information on how to provide an RDF file, containing a SHACL shape, to the LDES server can be found here. Fragmentation . To reduce the volume of data that consumers need to replicate or to speed up certain queries, the LDES server can be configured to create several fragmentations. Fragmentations are similar to indexes in databases but then published on the Web. The RDF predicate on which the fragmentation must be applied is defined through configuration. The fragmenting of a Linked Data Event Stream (LDES) is a crucial technique for managing and processing large amounts of data more efficiently. There are three main methods of fragmentation: geospatial, time-based, and substring fragmentation. . Partitioning . When applying partitioning, the LDES server will create fragments based on the order of arrival of the LDES member, and is a linear fragmentation. This fragmentation is considered the most basic and default fragmentation because it stands for the exact reason the LDES specification was created: replication and synchronising with a dataset. The members arriving on the LDES server are added to the first page, while the latest members are always included on the latest page. The expected parameter to apply a partioning is a member limit, indicating the amount of members that can be added to each page before creating a new page. name: “pagination” config: memberLimit: { Mandatory: member limit &gt; 0 } . Algorithm . | The fragment to which the member should be added is determined. | The currently open fragment is retrieved from the database. | If this fragment contains members equal to or exceeding the member limit or no fragment can be found, a new fragment is created instead. | . | If a new fragment is created, the following steps are taken. | The new fragment becomes the new open fragment and the previous fragment becomes immutable1. | This newly created fragment and the previous fragment are then linked with each other by 2 generic relationships1. | The pagenumber of the new fragment is determined based on the old fragment or is set to 1 in case of the first fragment. | . | . 1 In case of the first fragment, a previous fragment does not exist so these steps are skipped. Example properties . name: \"pagination\" config: memberLimit: 10 . Substring fragmentation . Substring fragmentation involves dividing the data stream into smaller pieces based on specific substrings, or patterns, within the data. Example of substring fragmentation configuration file . name: “substring” config: fragmenterProperty: { Defines which property will be used for bucketizing } memberLimit: { member limit &gt; 0 } . Example . Example properties: . name: \"substring\" config: fragmenterProperty: \"https://data.vlaanderen.be/ns/adres#volledigAdres\" memberLimit: 10 . With following example input: . @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;https://data.vlaanderen.be/id/adres/1781020/2023-02-15T10:14:36.002Z&gt; &lt;https://data.vlaanderen.be/ns/adres#isVerrijktMet&gt; [ &lt;https://data.vlaanderen.be/ns/adres#volledigAdres&gt; \"Kazernestraat 15, 9160 Lokeren\"@nl ] ; prov:generatedAtTime \"2023-02-15T10:14:36.002Z\"^^xsd:dateTime ; a &lt;https://data.vlaanderen.be/ns/adres#Adres&gt; . The selected object would be “Kazernestraat 15, 9160 Lokeren”. The bucket of substrings would be: . | K | Ka | Kaz | … | Kazernestraat 15, 9160 Lokeren | . If this is the first member of the collection it would be added to fragment ‘k’ and available at http://localhost:8080/addresses/by-name?substring=k and not in ka or any other substring fragment. In a scenario where there are already 10 addresses starting with ‘k’ and only 2 with ‘ka’, it would be added to http://localhost:8080/addresses/by-name?substring=ka . Note that this is all lowercase. . Time-based fragmentation . Time-based fragmentation has not yet been implemented. Example of a time-based fragmentation configuration file . name: “timebased” config: memberLimit: { member limit &gt; 0 } . Algorithm This fragmentiser will create an initial fragment with the current timestamp when processing a member. Members are added to the fragment until the member limit is reached. When the fragment member limit is reached, a next fragment is created with a new current timestamp. Reasons for deprecating this fragmentiser: . | This fragmentiser follows the algorithm of pagination but without the semantics. | For a correct timebased fragmentation, members of the fragment should be checked and their value for a given property should be used to create the correct relations. This is not the case, and there is currently no demand to have this implemented. | . Geospatial fragmentation . Consider the scenario where the address registry is published as an LDES that using partitioning. In such a case, data consumers are required to replicate the entire linear set of fragments, despite only being interested in a smaller subset of the dataset. For instance, the city of Brussels may only require addresses within its geographical region and is not interested in other addresses. However, with the partitioned LDES, they would need to iterate through all the fragments and filter the LDES members (address version objects) on the client-side. By utilising geospatial fragmentation, the data can be divided into smaller pieces (tiles) based on geographical location. This facilitates filtering on the fragment level (tiles) and allows for processing and analysis of data within specific geospatial tiles. The geospatial fragmentation supported by the LDES server is based on the “Slippy Maps” algorithm. The fragmentation expects a zoom level parameter which is used by the algorithm to divide the “world” into tiles. The number of tiles if 2^2n^ (where n = zoom level). The second expected parameter is an RDF predicate, indicating on which property of the LDES member the fragmentation should be applied. More information about the algorithm used to apply a geospatial fragmentation can be found here. The required configuration for this fragmentation is: . | RDF predicate on which the fragmentation should be based . | Zoom level . | . Example of geospatial fragmentation configuration file . name: “geospatial” config: maxZoomLevel: { Required zoom level } fragmenterProperty: { Defines which property will be used for bucketizing } . Algorithm . | The fragmentationObjects of the member are determined . | We filter the RDF statements where the predicate matches the fragmenterProperty | If an optional regex is provided through the fragmenterSubjectFilter property, we filter on subjects that match this regex. | We select all the object that pass the above filters. | . | A bucket of tiles is created using the coordinates and provided zoomLevel. This is done using the Slippy Map algorithm. | The tiles are iterated. The member is added to every tile, or sub-fragmentations of these tiles1. Taking into account: . | A new fragment is created if no fragment exists for the given tile. | There is no memberLimit or max size for a fragment. They do not become immutable. | The member is added to every related fragment | . | . 1 If the geospatial fragmentation is not the lowest fragmentation level, the member is not added to the tile but to a subfragment on this tile. This case is included in the example below. Example . Example properties: . name: \"geospatial\" config: maxZoomLevel: 15 fragmenterProperty: \"http://www.opengis.net/ont/geosparql#asWKT\" . With following example input: . @prefix dc: &lt;http://purl.org/dc/terms/&gt; . @prefix ns0: &lt;http://semweb.mmlab.be/ns/linkedconnections#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix ns1: &lt;http://vocab.gtfs.org/terms#&gt; . @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix ns2: &lt;http://www.opengis.net/ont/geosparql#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix geo: &lt;http://www.w3.org/2003/01/geo/wgs84_pos#&gt; . &lt;http://njh.me/original-id#2022-09-28T17:11:28.520Z&gt; dc:isVersionOf &lt;http://njh.me/original-id&gt; ; ns0:arrivalStop &lt;http://example.org/stops/402161&gt; ; ns0:arrivalTime \"2022-09-28T07:14:00.000Z\"^^xsd:dateTime ; ns0:departureStop &lt;http://example.org/stops/402303&gt; ; ns0:departureTime \"2022-09-28T07:09:00.000Z\"^^xsd:dateTime ; ns1:dropOffType ns1:Regular ; ns1:pickupType ns1:Regular ; ns1:route &lt;http://example.org/routes/Hasselt_-_Genk&gt; ; ns1:trip &lt;http://example.org/trips/Hasselt_-_Genk/Genk_-_Hasselt/20220928T0909&gt; ; a ns0:Connection ; prov:generatedAtTime \"2022-09-28T17:11:28.520Z\"^^xsd:dateTime . &lt;http://example.org/stops/402161&gt; ns2:asWKT \"POINT (5.47236 50.9642)\"^^ns2:wktLiteral ; a ns1:Stop ; rdfs:label \"Genk Brug\" ; geo:lat 5.096420e+1 ; geo:long 5.472360e+0 . &lt;http://example.org/stops/402303&gt; ns2:asWKT \"POINT (5.49661 50.9667)\"^^ns2:wktLiteral ; a ns1:Stop ; rdfs:label \"Genk Station perron 11\" ; geo:lat 5.096670e+1 ; geo:long 5.496610e+0 . The selected objects would be . \"POINT (5.47236 50.9642)\"^^ns2:wktLiteral and \"POINT (5.49661 50.9667)\"^^ns2:wktLiteral . When we convert these coordinates to tiles, the bucket of tiles would be: . | “15/16884/10974” | “15/16882/10975” | . When geospatial fragmentation is the lowest level . After ingestion the member will be part of the following two fragments . | http://localhost:8080/addresses/by-zone?tile=15/16884/10974 | http://localhost:8080/addresses/by-zone?tile=15/16882/10975 | . When we have a timebased sub-fragmentation below geospatial fragmentation . After ingestion the member will be part of the following two fragments . | http://localhost:8080/addresses/by-zone-and-time?tile=15/16884/10974&amp;generatedAtTime=2023-02-15T10:14:28.262Z | http://localhost:8080/addresses/by-zone-and-time?tile=15/16882/10975&amp;generatedAtTime=2023-02-15T10:14:28.262Z | . Note that the generatedAtTime=2023-02-15T10:14:28.262Z is an example, this can be any other fragmentation. . Combining geospatial fragmentation and partioning . The LDES server typically adds an LDES member to the “lowest” possible fragment, which in the case of geospatial fragmentation, would be the fragment representing the corresponding geospatial tile. However, some fragments/tiles may have many members, while others may have none. In the worst-case scenario, all members may be added to one geospatial tile/fragment, leading to an enormous fragment. Combining geospatial fragmentation with partitioning can be a useful approach to mitigate this issue. Then partitioning is applied within every geospatial tile, resulting in a set of linear fragments for every geospatial tile. Doing so, all members can still end up in the same geospatial tile, but now clients have a set of linear fragments to iterate through instead of one enormous fragment. More detailed information is available in the example below. Retention policy . A retention policy determines how long data will be kept and stored. Its purpose is to ensure the efficient use of storage resources by controlling data growth over time. Setting a retention policy per view to minimise storage fill-up is possible. Implementing a retention policy helps organisations maintain control over their data growth and ensure that storage resources are used optimally. The policy specifies the maximum duration that data should be kept. Time based retention policy . The time-based retention policy can be configured using the ISO 8601 duration format. This time-based policy ensures that data is automatically deleted after a specified period, freeing up valuable storage space for new data. @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix server: &lt;http://localhost:8080/mobility-hindrances/&gt; . server:time-based-retention tree:viewDescription [ ldes:retentionPolicy [ a ldes:DurationAgoPolicy ; tree:value \"PT5M\"^^xsd:duration ; ] ; ] . duration: “PT5M” . As an example, the time-based retention configuration example above is set up to ensure that data is automatically deleted after 5 minutes (PT5M). Point-in-time retention policy . The point in time retention policy of the Linked Data Event Stream (LDES) only preserves the members created after a specific moment. In this way, only the members made after a given point in time retain. @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix server: &lt;http://localhost:8080/mobility-hindrances/&gt; . server:point-in-time-retention tree:viewDescription [ ldes:retentionPolicy [ a ldes:PointInTimePolicy ; ldes:pointInTime \"2023-04-12T00:00:00\"^^xsd:dateTime ] ; ] . Version-based retention policy . The version-based retention policy of the system ensures that only the x most recent members of each state object are retained. In this way, only the x most recent members of each state object retain. @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix server: &lt;http://localhost:8080/mobility-hindrances/&gt; . server:version-based-retention tree:viewDescription [ ldes:retentionPolicy [ a ldes:LatestVersionSubset; ldes:amount 2 ; ] ; ] . Hosting the LDES stream SHACL shape . SHACL (Shapes Constraint Language) is a language used to validate RDF graphs against a set of conditions provided as shapes and other constructs in an RDF graph. The LDES Server facilitates hosting a SHACL shape describing the members in the LDES. Through configuration, it is possible to reference an existing SHACL shape via an URL or to provide a static file with an RDF description of the SHACL shape. Hosting DCAT metadata . DCAT is a standardised RDF vocabulary to describe data catalogues on the Web, allowing easy interoperability between catalogues. Using a standard schema, DCAT enhances discoverability and facilitates federated search across multiple catalogues. The LDES server facilitates hosting DCAT metadata when publishing an LDES. Through configuration, as with the SHACL shape, it is possible to reference an existing DCAT via an URI or to provide a static file containing an RDF description of the DCAT. More information on configuring DCAT on the LDES Server can be found here. Add DCAT configuration for the LDES server . @prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . [] a dcat:Catalog ; dct:title \"My LDES'es\"@en ; dct:description \"All LDES'es from publiser X\"@en . Add DCAT metadata for a LDES . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix dc: &lt;http://purl.org/dc/terms/&gt; . [] a dcat:Dataset ; dc:title \"My LDES\"@en ; dc:description \"LDES for my data collection\"@en . #### . ",
    "url": "/introduction/LDES_server#setting-up-the-ldes-server-during-startup-process",
    
    "relUrl": "/introduction/LDES_server#setting-up-the-ldes-server-during-startup-process"
  },"12": {
    "doc": "LDES Server",
    "title": "Setting up the LDES Server using API",
    "content": "Setup of the LDES Server . To start a default LDES Server, a few basic steps are needed. | Create a ldes-server.yml config file with this basic content | . mongock: migration-scan-package: VSDS springdoc: swagger-ui: path: /v1/swagger ldes-server: host-name: \"http://localhost:8080\" management: tracing: enabled: false spring: data: mongodb: database: ldes host: ldes-mongodb port: 27017 auto-index-creation: true . | Create a local docker-compose.yml file with the content below. | . version: \"3.3\" services: ldes-server: container_name: basic_ldes-server image: ghcr.io/informatievlaanderen/ldes-server:20230602200451 environment: - SPRING_CONFIG_LOCATION=/config/ volumes: - ./ldes-server.yml:/config/application.yml:ro ports: - 8080:8080 networks: - ldes depends_on: - ldes-mongodb ldes-mongodb: container_name: quick_start_ldes-mongodb image: mongo:6.0.4 ports: - 27017:27017 networks: - ldes networks: ldes: name: quick_start_network . | Run docker compose up within the work directory of docker-compose.yml file to start the containers. | . Setting up metadata for the server . Setting up metadata for your LDES Server can be done by posting a RDF object defining a DCAT catalog to /admin/api/v1/dcat . @prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; . @prefix org: &lt;http://www.w3.org/ns/org#&gt; . @prefix legal: &lt;http://www.w3.org/ns/legal#&gt; . @prefix m8g: &lt;http://data.europa.eu/m8g/&gt; . @prefix locn: &lt;http://www.w3.org/ns/locn#&gt; . [] a dcat:Catalog ; dct:title \"My LDES'es\"@en ; dct:description \"All LDES'es from publiser X\"@en ; dct:publisher &lt;http://sample.org/company/PublisherX&gt; . &lt;http://sample.org/company/PublisherX&gt; a legal:LegalEntity ; foaf:name \"Data Publishing Company\" ; legal:legalName \"Data Publishing Company BV\" ; m8g:registeredAddress [ a locn:Address ; locn:fullAddress \"Some full address here\" ] ; m8g:contactPoint [ a m8g:ContactPoint ; m8g:hasEmail \"info@data-publishing-company.com\" ] . This can be updated by performing a PUT operation with an updated DCAT catalog on /admin/api/v1/dcat/{catalogID} . Finally, to delete the catalog, a DELETE request can be performed at /admin/api/v1/dcat/{catalogID} . Further documentation can be found on the internal Swagger API available at `/v1/swagger` . Setting up a collection . Setting up a collection on the LDES Server can be done by posting a RDF object defining a collection to /admin/api/v1/eventstreams . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix custom: &lt;http://example.org/&gt; . @prefix dcterms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix sh: &lt;http://www.w3.org/ns/shacl#&gt; . @prefix server: &lt;http://localhost:8080/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . server:exampleCollection a ldes:EventStream ; ldes:timestampPath dcterms:created ; ldes:versionOfPath dcterms:isVersionOf ; custom:memberType &lt;https://data.vlaanderen.be/ns/mobiliteit#Mobiliteitshinder&gt; ; custom:hasDefaultView \"true\"^^xsd:boolean ; tree:shape [ sh:closed \"true\"; a sh:NodeShape ; ] . This collection can be deleted by performing a DELETE request on /admin/api/v1/eventstreams/{collectionName} . Further documentation can be found on the internal Swagger API available at `/v1/swagger` . Setting up metadata for collection . To add metadata to an inserted collection, one can post a DCAT dataset on /admin/api/v1/eventstreams/{collectionName}/dcat . @prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; . @prefix org: &lt;http://www.w3.org/ns/org#&gt; . @prefix legal: &lt;http://www.w3.org/ns/legal#&gt; . @prefix m8g: &lt;http://data.europa.eu/m8g/&gt; . @prefix locn: &lt;http://www.w3.org/ns/locn#&gt; . [] a dcat:Dataset ; dct:title \"My LDES\"@en ; dct:title \"Mijn LDES\"@nl ; dct:description \"LDES for my data collection\"@en ; dct:description \"LDES vir my data-insameling\"@af ; dct:creator &lt;http://sample.org/company/MyDataOwner&gt; . &lt;http://sample.org/company/MyDataOwner&gt; a legal:LegalEntity ; foaf:name \"Data Company\" ; legal:legalName \"Data Company BV\" ; m8g:registeredAddress [ a locn:Address ; locn:fullAddress \"My full address here\" ] ; m8g:contactPoint [ a m8g:ContactPoint ; m8g:hasEmail \"info@data-company.com\" ] . To update this entry, a PUT request can be performed on /admin/api/v1/eventstreams/{collectionName}/dcat. Similarly, a DELETE request can be performed on /admin/api/v1/eventstreams/{collectionName}/dcat . Further documentation can be found on the internal Swagger API available at `/v1/swagger` . Setting up a view . Setting up a view on the LDES Server can be done by performing a POST operation with a RDF object defining a collection to /admin/api/v1/eventstreams/{collectionName}/views . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix example: &lt;http://example.org/&gt; . @prefix server: &lt;http://localhost:8080/name1/&gt; . @prefix viewName: &lt;http://localhost:8080/name1/view1/&gt; . viewName:description a &lt;https://w3id.org/tree#ViewDescription&gt; ; ldes:retentionPolicy [ a ldes:retentionPolicy ; example:name \"timebased\"; example:duration \"10\" ; ] . server:view1 &lt;https://w3id.org/tree#viewDescription&gt; &lt;http://localhost:8080/name1/view1/description&gt; . Further documentation can be found on the internal Swagger API available at `/v1/swagger` . Setting up metadata for view . To add metadata to an inserted view, one can perform a PUT operation with a DCAT view description and dataservice on /admin/api/v1/eventstreams/{collectionName}/views/{viewName}/dcat . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix dc: &lt;http://purl.org/dc/terms/&gt; . @prefix host: &lt;http://localhost:8080/&gt; . @prefix server: &lt;http://localhost:8080/collection/&gt; . @prefix viewName: &lt;http://localhost:8080/collection/viewName/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . server:viewName a tree:Node ; tree:viewDescription viewName:description . viewName:description a dcat:DataService , tree:ViewDescription ; tree:fragmentationStrategy ([ a tree:ExampleFragmentation ; tree:pageSize \"100\" ; tree:property \"example/property\" ]) ; dc:description \"Geospatial fragmentation for my LDES\"@en ; dc:title \"My geo-spatial view\"@en ; dc:license [ a dc:LicenseDocument ; dc:type [ a skos:Concept ; skos:prefLabel \"some public license\"@en ] ] ; ldes:retentionPolicy [ a ldes:DurationAgoPolicy ; tree:value \"PT2M\"^^xsd:duration ; ] ; dcat:endpointURL server:viewName ; dcat:servesDataset host:collection ; . Similarly, a DELETE request can be performed on /admin/api/v1/eventstreams/{collectionName}/views/{viewName}/dcat . Further documentation can be found on the internal Swagger API available at `/v1/swagger` . Setting up ACM/IDM . The Access and User Management (ACM) and Identity Management (IDM) of the Flemish government are products that allow you to manage the access and identity of data users that consume the published LDES. This ADM/IDM security option through an API gateway, protects LDES Collections and Views from unauthorized access, recognizing the importance of data security. The API gateway serves as a security layer, managing access and applying authentication methods, such as ACM/IDM, reducing the chance of exposing sensitive data to unwanted parties. ACM/IDM verifies the identity and permissions of users and devices. This improved security feature increases the trust and dependability of LDES Server for organizations working in security-sensitive environments. During the ACM/IDM Standard integration process, you will work alongside the ACM/IDM integration team of the Flemish government to follow the standard connection procedure. One of our analysts will guide you through this process via an integration file, where agreements and requirements are documented. This procedure is followed for both new files and modifications to existing files. For more information, you can find it here . OpenAPI swagger UI . Via the OpenAPI Specification it becomes possible discover how the LDES server API works, how to configure the LDES server, etc., in a user-friendly manner. As an example, the Swagger API docs can be find here. The Swagger API should look like this: . | ",
    "url": "/introduction/LDES_server#setting-up-the-ldes-server-using-api",
    
    "relUrl": "/introduction/LDES_server#setting-up-the-ldes-server-using-api"
  },"13": {
    "doc": "Specification",
    "title": "Specification",
    "content": " ",
    "url": "/introduction/Specification",
    
    "relUrl": "/introduction/Specification"
  },"14": {
    "doc": "Specification",
    "title": "What is a Linked data Event Stream?",
    "content": "The Linked Data Event Stream (LDES) specification (ldes:EventStream) allows data publisher to publish their dataset as an append-only collection of immutable members in its most basic form. Consumers can host one or more in-sync views on top of the default (append-only) view. An LDES is defined as a collection of immutable objects, often referred to as LDES members. These members are described using a specific format called RDF, which stands for Resource Description Framework. RDF is one of the corner stones of Linked Data and on which LDES continues to build. More information on Linked Data can be found here. The LDES specification is based on a hypermedia specification, called the TREE specification. The TREE specification originates from the idea to provide an alternative to one-dimensional HTTP pagination. It allows to fragment a collection of items and interlink these fragments. Instead of linking to the next or previous page, the relation describes what elements can be found by following the link to another fragment. The LDES specification extends the TREE specification by stating that every item in the collection must be immutable. The TREE specification is compatible with other specifications such as activitystreams-core, VOCAB-DCAT-2, LDP, or Shape Trees. For specific compatibility rules, please refer to the TREE specification. LDESes apply — as the term implies — the Linked Data principles to data event streams. A data stream is typically a constant flow of distinct data points, each containing information about an event or change of state that originates from a system that continuously creates data. Some examples of data streams include sensor and other IoT data, financial data, etc. Today, custom code has to be created to integrate data, which makes it rather expensive to integrate multiple data sources. With LDES, a technical standard was created that allows data to be exchanged across silos using domain-specific ontologies. An LDES allows third parties to build specific services (WFS, SPARQL endpoint) themselves on top of their own database that is always in sync with the original dataset. An LDES is a constant flow of immutable objects (such as version objects of addresses, sensor observations or archived representations) containing information changes that originates from a system that continuously creates data. Compared to other event stream specification, the LDES specs opts to publish the entire object for every change. Furthermore, LDES increases the usability and findability of data, as it comes in a uniform Linked Data standard published on an online URI endpoint. As a result, an LDES is self-descriptive meaning and more data can be found by following the links. In a nutshell, there are several reasons why there was a need to develop the Linked Data Event Streams specification: . | Linked Data is a powerful paradigm for representing and sharing data on the Web. Still, it has traditionally focused on representing static data rather than events or changes to that data. | The use of event streams is becoming increasingly prevalent on the Web, as it enables applications to exchange information about changes to data in real-time efficiently. | There was a need for a semantic standard that provides a uniform way to exchange data so that different systems could easily exchange data. | Linked Data Event Streams allow applications to subscribe to a stream of data and receive updates in real-time. | . ",
    "url": "/introduction/Specification#what-is-a-linked-data-event-stream",
    
    "relUrl": "/introduction/Specification#what-is-a-linked-data-event-stream"
  },"15": {
    "doc": "Specification",
    "title": "Structure of a Linked Data Event Stream",
    "content": "As defined above, an LDES is a collection of members or immutable objects. The LDES spec works both for fast moving data and slow moving data. An example of fast moving data, such as sensor observations, is shown in the example below. The base URI for LDES is https://w3id.org/ldes#, with the preferred prefix being `ldes`. @prefix ldes: &lt;http://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix sosa: &lt;http://www.w3.org/ns/sosa/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;C1&gt; a ldes:EventStream ; ldes:timestampPath sosa:resultTime ; tree:shape &lt;C1/shape.shacl&gt; ; tree:member &lt;observation1&gt; . &lt;observation1&gt; a sosa:Observation ; sosa:resultTime \"2021-01-01T00:00:00Z\"^^xsd:dateTime ; sosa:hasSimpleResult \"...\" . The observation entity (`&lt;observation1&gt;`) is considered to be immutable, and its existing identifiers can be utilized as such. The specification indicates that an ldes:EventStream should have the following properties: . | tree:member → indicating the members of the collection | tree:shape → a machine-readable description of the members in the collection. Can be SHACL or ShEx. | . Otherwise, an ldes:EventStream may have these properties: . | ldes:timestampPath → indicates how a member precedes another member in the LDES, using a timestamp. | ldes:versionOfPath → indicating the non-version object. See example in the specification. | . As stated above, an LDES can also publish a slow moving dataset, such as street names. An example is shown below. @prefix ldes: &lt;http://w3id.org/ldes#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix dcterms: &lt;http://purl.org/dc/elements/1.1/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix sosa: &lt;http://www.w3.org/ns/sosa/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;C1&gt; a ldes:EventStream ; tree:shape &lt;C1/shape.shacl&gt; ; tree:member &lt;streetname1-v1&gt;, &lt;streetname1-v2&gt; . &lt;streetname1-v1&gt; rdfs:label \"Station Road\" ; dcterms:isVersionOf &lt;streetname1&gt; ; dcterms:created \"2020-01-01T00:10:00Z\"^^xsd:dateTime . &lt;streetname1-v2&gt; rdfs:label \"Station Square\" ; dcterms:isVersionOf &lt;streetname1&gt; ; dcterms:created \"2021-01-01T00:10:00Z\"^^xsd:dateTime . This example introduces the concept of versions, because certain entities, such as street names, do not understand the concept of time. In this example, versions of street names are published, ensuring the immutability of the LDES members. When publishing versions of entities, extra information (dcterms:isVersionOf) must be added in order to be able to link these version to an entity. Not introducing versions for entities that do not understand the concept of time would lead to an incorrect implementation of the LDES spec, as shown below. @prefix ldes: &lt;http://w3id.org/ldes#&gt; . @prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; . @prefix dcterms: &lt;http://purl.org/dc/elements/1.1/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix sosa: &lt;http://www.w3.org/ns/sosa/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;C1&gt; a ldes:EventStream ; tree:shape &lt;C1/shape.shacl&gt; ; tree:member &lt;streetname1&gt; . &lt;streetname1&gt; rdfs:label \"Station Road\" ; dcterms:created \"2020-01-01T00:10:00Z\"^^xsd:dateTime . &lt;streetname1&gt; rdfs:label \"Station Square\" ; dcterms:created \"2021-01-01T00:10:00Z\"^^xsd:dateTime . In this example, the entity with HTTP URI &lt;streetname1&gt; is not longer immutable, which is a direct conflict with the definition of the LDES spec. It is important to note that once a client processes a member of an LDES, it should never have to process it again. Therefore, a Linked Data Event Stream client can maintain a list of already processed member IRIs in a cache. A reference implementation of a client is available as an open-source [SDK](https://github.com/Informatievlaanderen/VSDS-Linked-Data-Interactions/tree/main/ldi-core#1-ldes-client) as part of the Flanders Smart Data Space initiative. ",
    "url": "/introduction/Specification#structure-of-a-linked-data-event-stream",
    
    "relUrl": "/introduction/Specification#structure-of-a-linked-data-event-stream"
  },"16": {
    "doc": "Specification",
    "title": "Features of the LDES spec",
    "content": "Fragmentation and pagination . An LDES focuses on allowing clients to replicate a dataset’s history and efficiently synchronise with its latest changes. Linked Data Event Streams may be fragmented when their size becomes too big for one HTTP response. Fragmenting an LDES has two main advantages: . | It speeds up certain queries. E.g. an autocompletion client will solve its queries faster using a substring fragmentation than a lineair (append-only) fragmentation | It allows data consumers to replicate/stay in sync with only the part of the dataset they are actually interested in. | . The most basic fragmentation of an LDES is called partitioning, which creates a linear fragmentation, appending the newest members on the latest fragment. @prefix ldes: &lt;http://w3id.org/ldes#&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;C1&gt; a ldes:EventStream ; ldes:timestampPath sosa:resultTime ; tree:shape &lt;C1/shape.shacl&gt; ; tree:member &lt;observation1&gt;, ... ; tree:view &lt;?page=1&gt; . &lt;?page=1&gt; a tree:Node ; tree:relation [ a tree:GreaterThanOrEqualToRelation ; tree:path sosa:resultTime ; tree:node &lt;?page=2&gt; ; tree:value \"2020-12-24T12:00:00Z\"^^xsd:dateTime ] . Each relation to another fragment is semantically described, helping clients to decide whether or not it is interesting to follow the relation. The LDES server building block implements various fragmentations. More information can be found [here](https://informatievlaanderen.github.iodocs/LDES_server.html#fragmentation). Retention policy . A retention policy is a set of rules determining how long data should be kept or deleted. A retention policy can be applied to an LDES to manage the storage and availability of data objects over time. Currently, the LDES spec defines two retention policies, a time-based an a version-based retention policy. More information about the retention policies can be found in the spec. The LDES Server buildling block implements a time-based retention policy. More information can be found [here](https://informatievlaanderen.github.iodocs/LDES_server.html#retention-policy). ",
    "url": "/introduction/Specification#features-of-the-ldes-spec",
    
    "relUrl": "/introduction/Specification#features-of-the-ldes-spec"
  },"17": {
    "doc": "Specification",
    "title": "SHACL",
    "content": "SHACL (Shapes Constraint Language) is a standard for validating RDF data and ensuring that it conforms to a particular structure or shape. In the context of the Linked Data Event Stream (LDES), SHACL shapes are used to provide a machine-readble description of the expected structure of members in the stream. By incorporating SHACL shapes, LDES provides a powerful tool for ensuring data quality and consistency, making it a reliable and trustworthy source of data for various applications. By defining a SHACL shape for the LDES, data producers can ensure that the members they add to the LDES adhere to the required structure, while data consumers can use the shape to validate and reason about the data they receive. As a consequence of the immutability of the members, this shape *may* evolve, but it **must** always be backwards compatible to the earlier version. When the new shape is not backwards compatible, a new LDES must be created. ",
    "url": "/introduction/Specification#shacl",
    
    "relUrl": "/introduction/Specification#shacl"
  },"18": {
    "doc": "Specification",
    "title": "DCAT",
    "content": "DCAT is an RDF vocabulary for data catalogues on the Web, enabling easy interoperability and discoverability of metadata for datasets, data services, and portals. It standardises properties for describing datasets, access information, and data services. By using DCAT, publishers can increase their datasets’ exposure and facilitate data sharing and reuse. The LDES Server building block allows to pass a static RDF file on startup, containing DCAT to describe the LDES(es). The server reads and publishes the content. ",
    "url": "/introduction/Specification#dcat",
    
    "relUrl": "/introduction/Specification#dcat"
  },"19": {
    "doc": "LDES2Service",
    "title": "Triple stores",
    "content": ". The LDES2Service toolbox contains an “RDF4J Put” processor, allowing to ingest members in triple stores that support the RDF4J API. In this Github repo, a docker-compose file with the configuration of GraphDB and Apache NiFi. A data flow is configured in Apache NiFi to replicate LDES members into GraphDB. There is already a preconfigured Apache Nifi workflow file available. Execute the following steps to start replicating data into GraphDB. | Start Docker . | Make sure port 8433 (Apache Nifi) and 7200 (GraphDB) are accessible . | Create a local docker-compose.yml file with the following good-to-go configuration | Run docker-compose up --build to start the Apache Nifi and GraphDB containers . | Configure an LDES endpoint in the LDES Client in Apache Nifi . | Go to your GraphDB database via http://localhost:7200/ . | Create a repo in GraphDB . | Start the Apache NiFi workflow . | LDES members will be replicated one by one into your GraphDB | . You can find more information in this article . ",
    "url": "/quickstart/LDES2services#triple-stores",
    
    "relUrl": "/quickstart/LDES2services#triple-stores"
  },"20": {
    "doc": "LDES2Service",
    "title": "PostgreSQL &amp; TimescaleDB",
    "content": ". In this Github repo, you will find a docker file with the configuration of TimescaleDB and Apache NiFi. A workflow is configured in Apache NiFi to replicate LDES members into TimescaleDB. You will find an Apache NiFi configuration file containing the necessary data flow. This Apache NiFi data flow works also for storing LDES members in a PostgreSQL database. | Start Docker . | Make sure port 8433, 8001 and 8002 are accessible . | Create a local docker-compose.yml file with the following good-to-go configuration . | Run docker-compose up --build . | import the workflow in Apache NiFi (localhost:8443) via this configuration file . | Configure an LDES endpoint in the LDES Client . | Go to your PgAdmin to access the TimescaleDB database via localhost:8002/ . | Create a database in TimescaleDB . | Start the Apache NiFi workflow . | LDES members will be replicated one by one into your TimescaleDB . | . You can find more information in this article . ",
    "url": "/quickstart/LDES2services#postgresql--timescaledb",
    
    "relUrl": "/quickstart/LDES2services#postgresql--timescaledb"
  },"21": {
    "doc": "LDES2Service",
    "title": "PowerBI",
    "content": ". In this Github repo, you will find a docker file with the configuration of PostgreSQL, Apache NiFi and a configuration file for PowerBI. A workflow is configured in Apache NiFi to replicate the LDES members into PostgreSQL. You will find an Apache NiFi configuration file containing the necessary data flow. | Start Docker . | Make sure port 8433, 8001 and 8002 are accessible . | Create a local docker-compose.yml file with the following good-to-go configuration . | Run docker-compose up --build . | import the workflow in Apache NiFi (localhost:8443) via this configuration file . | Configure an LDES endpoint in the LDES Client . | Go to your PgAdmin to access your PostgreSQL database via localhost:8002/ . | Create a database in PostgreSQL . | Start the Apache NiFi workflow . | LDES members will be replicated one by one into your PostgreSQL . | Link PostgreSQL to Geoserver . | . You can find more information in this article . ",
    "url": "/quickstart/LDES2services#powerbi",
    
    "relUrl": "/quickstart/LDES2services#powerbi"
  },"22": {
    "doc": "LDES2Service",
    "title": "GeoServer",
    "content": ". In this Github repo, you will find a docker file with the configuration of PostgreSQL, Apache NiFi and Geoserver. A data flow is configured in Apache NiFi to convert these data streams into PostgreSQL. You will find an Apache NiFi configuration file containing the necessary data flow. This Apache NiFi data flow works also for storing LDES members in a PostgreSQL database. | Install Docker . | Make sure port 8433, 8001 and 8002 are accessible . | Create a local docker-compose.yml file with the following good-to-go configuration . | Run docker-compose up --build . | import a data flow in Apache NiFi (localhost:8443) via this configuration file | Assign and LDES endpoint in the LDES client . | Go to PgAdmin to access your PostgreSQL database via localhost:8002/ . | Create a database in PostgreSQL . | Start the Apache NiFi data flow . | LDES members will be stored one by one in your PostgreSQL . | Link PostgreSQL to Geoserver | . You can find more information in this article . ",
    "url": "/quickstart/LDES2services#geoserver",
    
    "relUrl": "/quickstart/LDES2services#geoserver"
  },"23": {
    "doc": "LDES2Service",
    "title": "LDES to QGIS",
    "content": ". | Follow the steps from LDES to PostgreSQL/TimescaleDB . | Connecting QGIS to a PostgreSQL/PostGIS database lets you access and visualise your geo data in real-time. | . You can find more information in this article . ",
    "url": "/quickstart/LDES2services#ldes-to-qgis",
    
    "relUrl": "/quickstart/LDES2services#ldes-to-qgis"
  },"24": {
    "doc": "LDES2Service",
    "title": "Machine Learning (ML-LDES server)",
    "content": "This is a prototype!! . In this Github repo, you will find a docker file with the configuration of PostgreSQL and Apache NiFi. A data flow is configured in Apache NiFi to convert these data streams into PostgreSQL. You will find an Apache NiFi configuration file containing the necessary data flow. This Apache NiFi data flow works also for storing LDES members in a PostgreSQL database. | Install Python requirements via: pip install -r requirements.txt . | Install Docker . | Make sure port 8433, 8001 and 5432 are accessible . | Create a local docker-compose.yml file with the following good-to-go configuration . | Run docker-compose up --build . | import a data flow in Apache NiFi (localhost:8443) via this configuration file . | Assign and LDES endpoint in the LDES client . | Go to PgAdmin to access your TimescaleDB database (localhost:8002/) . | Create a database in TimescaleDB . | Start the Apache NiFi data flow . | LDES members will be stored one by one in your TimescaleDB . | . Go to github repo for more info about ML-LDES server prototype . You can find more information in this and this article . ",
    "url": "/quickstart/LDES2services#machine-learning-ml-ldes-server",
    
    "relUrl": "/quickstart/LDES2services#machine-learning-ml-ldes-server"
  },"25": {
    "doc": "LDES2Service",
    "title": "LDES2Service",
    "content": "This sections provides an overview of how data consumer can replicate data to different backend systems. ",
    "url": "/quickstart/LDES2services",
    
    "relUrl": "/quickstart/LDES2services"
  },"26": {
    "doc": "Use case LDES server",
    "title": "Use case LDES server",
    "content": "Having trouble implementing these examples? Please post your issue on the VSDS Tech docs repo. Apache Kafka, Fiware-Orien Context Broker, and MQTT can be used as a Publisher to the VSDS LDES Ecosystem. The following examples will explain the use cases. ",
    "url": "/quickstart/Use_case_LDES_server",
    
    "relUrl": "/quickstart/Use_case_LDES_server"
  },"27": {
    "doc": "Use case LDES server",
    "title": "Kafka to LDES server",
    "content": "Apache Kafka can be used as a data provider for ingesting data topics into the LDES ecosystem. The diagram below illustrates how the VSDS NIFI solution subscribes to a Kafka stream, updates the dataset’s attributes, converts it into an LDES-formatted stream, and uses HTTP protocols to publish it to the LDES Server. This process enables fragmentation or pagination of the data. This GitHub repository demonstrates the configuration of transferring subscribed GRAR (Building units, addresses &amp; parcels) Kafka data stream to the published substring fragmented LDES Stream using LDES Server. As we have no control over the GRAR system, the demo uses a JSON Data Generator which produces a continuous stream of addresses as an alternative to the GRAR system. Also, the Apache NIFI has standard Kafka Reader processors for subscribing to Kafka stream, please modify the nifi-workflow.json accordingly based on your environment. An example setup with Kafka can be as follow (GRAR.json), please modify the credentials for the Kafka topic accordingly: . To try out the demo, you need to make sure the required ports for LDES Server and NIFI are free to be used. For the details, please refer to docker-compose.yml. The steps are composed of the following steps: . 1. Docker run start all required docker images. 2. Upload a pre-defined NiFi workflow. 3. Start the NiFi workflow. 4. Start the address ingestion. (Modify according to your Kafka setup) . Please follow README.md for step-by-step guide. ",
    "url": "/quickstart/Use_case_LDES_server#kafka-to-ldes-server",
    
    "relUrl": "/quickstart/Use_case_LDES_server#kafka-to-ldes-server"
  },"28": {
    "doc": "Use case LDES server",
    "title": "MQTT to LDES server",
    "content": ". ",
    "url": "/quickstart/Use_case_LDES_server#mqtt-to-ldes-server",
    
    "relUrl": "/quickstart/Use_case_LDES_server#mqtt-to-ldes-server"
  },"29": {
    "doc": "Use case LDES server",
    "title": "Fiware to LDES server",
    "content": "The FIWARE-Orion Context Broker (OCB) can be integrated as a data provider with the VSDS LDES (Linked Data Event Streams) Server. The OCB is an open-source software component developed by FIWARE that can manage real-time context information by receiving updates from IoT devices, sensors, and other sources and storing this information in a centralized location. One example of this integration is demonstrated in the diagram below, which illustrates the use case of onboarding the Internet of Water (VMM) data. The details of this use case locate at Orien Context Broker - IOW. In this case, the OCB is integrated into the LDES ecosystem to publish context updates to an LDES stream. The VSDS NIFI solution is used to translate the context data into LDES events and publish them to the LDES stream via an update attributes processor, an OSLO converter processor, and an LdesConverter process NIFI pipeline. Once the context updates are published to the LDES Sever in LDES formatted stream, they can be processed and stored in the LDES Server as linked data. This makes the context information available for further analysis and uses in other systems. An example NIFI setup with Fiware-Orien Context Broker can be as follow, which locates at workflow.json. Please follow README.md for step-by-step guide. ",
    "url": "/quickstart/Use_case_LDES_server#fiware-to-ldes-server",
    
    "relUrl": "/quickstart/Use_case_LDES_server#fiware-to-ldes-server"
  },"30": {
    "doc": "Quick start",
    "title": "Quick start",
    "content": " ",
    "url": "/quickstart/quickstart",
    
    "relUrl": "/quickstart/quickstart"
  },"31": {
    "doc": "Quick start",
    "title": "LDES Server &amp; LDES Client",
    "content": "This example focuses on both publishing and consuming a Linked Data Event Stream (LDES). We start by explaining how to setup an LDES server and publish data as an LDES, followed by the setup of the LDES client to replicate an LDES. In this example, the data examples are described with OSLO (the Flemish Interoperability Program) ontologies. The files you will use for this quick start guide is available here . This quick start example demonstrates only a small amount of the capabilities of the LDES Server. For more information about the LDES server, please consult the LDES Server Manual. Before starting . | Make sure Docker has been installed on your device. | Local Ports 8080, and 27017 are accessible. | The command script is written in bash code. Please modify it accordingly. | . Setup an LDES Server . To start a default LDES Server, a few basic steps are needed. | Create a ldes-server.yml config file with this basic content . mongock: migration-scan-package: VSDS springdoc: swagger-ui: path: /v1/swagger ldes-server: host-name: \"http://localhost:8080\" management: tracing: enabled: false spring: data: mongodb: database: ldes host: ldes-mongodb port: 27017 auto-index-creation: true . | Create a local docker-compose.yml file with the content below. version: \"3.3\" services: ldes-server: container_name: basic_ldes-server image: ldes/ldes-server:1.0.0-SNAPSHOT environment: - SPRING_CONFIG_LOCATION=/config/ volumes: - ./ldes-server.yml:/config/application.yml:ro ports: - 8080:8080 networks: - ldes depends_on: - ldes-mongodb ldes-mongodb: container_name: quick_start_ldes-mongodb image: mongo:6.0.4 ports: - 27017:27017 networks: - ldes ldio-workbench: container_name: basic_ldes-replication image: ldes/ldi-orchestrator:1.0.0-SNAPSHOT environment: - SPRING_CONFIG_NAME=application - SPRING_CONFIG_LOCATION=/config/ volumes: - ./ldio.yml:/config/application.yml:ro ports: - ${LDIO_WORKBENCH_PORT:-8081}:8080 networks: - ldes profiles: - delay-started networks: ldes: name: quick_start_network . | Run docker compose up within the work directory of .yml file, to start the containers. | The LDES Server is now available at port 8080 and accepts members via HTTP POST requests. | We will now configure the LDES Server. (note that this part can also be done with the Swagger endpoint (/v1/swagger), where more detailed documentation is available) . | Let’s set the DCAT metadata for the server by defining a title and a description: | . curl -X 'POST' \\ 'http://localhost:8080/admin/api/v1/dcat' \\ -H 'accept: text/plain' \\ -H 'Content-Type: text/turtle' \\ ---data-raw ' @prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . [] a dcat:Catalog ; dct:title \"My LDES'\\''es\"@en ; dct:description \"All LDES'\\''es from publiser X\"@en . ' . | Next, let’s add collection for our mobility hindrances. This will be used to replicate the dataset from GIPOD: . | A collection name “mobility-hindrances” | Will process members of type “https://data.vlaanderen.be/ns/mobiliteit#Mobiliteitshinder” | Will have 2 views : . | A default view, which provides a basic view using a paginated fragmention. This also enables snapshotting. (this view is called by-page by default) | A time-based fragmented view. This will fragment the members based on their timebased property. This value is by default set to http://www.w3.org/ns/prov#generatedAtTime | . | . curl -X 'PUT' 'http://localhost:8080/admin/api/v1/eventstreams' \\ -H 'accept: text/turtle' \\ -H 'Content-Type: text/turtle' \\ --data-raw '@prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix dcterms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix sh: &lt;http://www.w3.org/ns/shacl#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix example: &lt;http://example.org/&gt; . @prefix collection: &lt;/mobility-hindrances/&gt; . &lt;/mobility-hindrances&gt; a ldes:EventStream ; ldes:timestampPath dcterms:created ; ldes:versionOfPath dcterms:isVersionOf ; example:memberType &lt;https://data.vlaanderen.be/ns/mobiliteit#Mobiliteitshinder&gt; ; example:hasDefaultView \"true\"^^xsd:boolean ; ldes:view collection:time-based ; tree:shape [ a sh:NodeShape ; ] . collection:time-based tree:viewDescription [ ldes:retentionPolicy [ a ldes:DurationAgoPolicy ; tree:value \"PT2M\"^^xsd:duration ; ] ; tree:fragmentationStrategy ([ a tree:Fragmentation ; tree:name \"timebased\" ; tree:memberLimit \"20\" ; ]) ; ] . ' . | Let’s add some DCAT metadata for this collection: . | We define an English and Dutch title and description | We define the creator of the collection to be http://sample.org/company/MyDataOwner | . curl -X 'POST' \\ 'http://localhost:8080/admin/api/v1/eventstreams/mobility-hindrances/dcat' \\ -H 'accept: */*' \\ -H 'Content-Type: text/turtle' \\ -d ' @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . @prefix dct: &lt;http://purl.org/dc/terms/&gt; . [] a dcat:Dataset ; dct:title \"Mobility Hindrances Collection\"@en ; dct:title \"Mobiliteitshindernissen collectie\"@nl ; dct:description \"A collection containing mobility hindrances\"@en ; dct:description \"Een collectie met Mobiliteitshindernissen\"@nl ; dct:creator &lt;http://sample.org/company/MyDataOwner&gt; ;' . | We’ll now also set the DCAT metadata for the time-based view by defining a title and a description : . curl -X 'POST' \\ 'http://localhost:8080/admin/api/v1/eventstreams/mobility-hindrances/views/time-based/dcat' \\ -H 'accept: */*' \\ -H 'Content-Type: text/turtle' \\ -d '@prefix dct: &lt;http://purl.org/dc/terms/&gt; . @prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; . [] a dcat:DataService ; dct:title \"My timebased view\"@en ; dct:description \"Timebased fragmentation for the mobility-hindrances \"@en . ' . | Next, let’s add a second collection to collect observations: . | A collection name “observations” | Will process members of type “https://data.vlaanderen.be/ns/mobiliteit#ObservationCollection” | Will have a default view, which provides a basic view using a paginated fragmention. This also enables snapshotting. | . curl -X 'PUT' 'http://localhost:8080/admin/api/v1/eventstreams' \\ -H 'accept: text/turtle' \\ -H 'Content-Type: text/turtle' \\ --data-raw '@prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix example: &lt;http://example.org/&gt; . @prefix dcterms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt;. @prefix sh: &lt;http://www.w3.org/ns/shacl#&gt; . @prefix server: &lt;http://localhost:8080/&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . server:observations a ldes:EventStream ; ldes:timestampPath dcterms:created ; ldes:versionOfPath dcterms:isVersionOf ; example:memberType &lt;https://data.vlaanderen.be/ns/mobiliteit#ObservationCollection&gt; ; example:hasDefaultView \"true\"^^xsd:boolean ; tree:shape [ a sh:NodeShape ; ] . ' . | . Add data to the LDES Server . | Create a observation.ttl file with the following content: . @prefix dc: &lt;http://purl.org/dc/terms/&gt; . @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . @prefix sosa: &lt;http://www.w3.org/ns/sosa/&gt; . @prefix ns0: &lt;http://def.isotc211.org/iso19156/2011/SamplingFeature#SF_SamplingFeatureCollection.&gt; . @prefix ns1: &lt;http://def.isotc211.org/iso19156/2011/Observation#OM_Observation.&gt; . @prefix ns2: &lt;http://def.isotc211.org/iso19103/2005/UnitsOfMeasure#Measure.&gt; . @prefix ns3: &lt;https://schema.org/&gt; . &lt;urn:ngsi-ld:WaterQualityObserved:woq:1/2023-03-12T18:31:17.003Z&gt; dc:isVersionOf &lt;urn:ngsi-ld:WaterQualityObserved:woq:1&gt; ; prov:generatedAtTime \"2023-03-12T18:31:17.003Z\"^^xsd:dateTime ; sosa:hasFeatureOfInterest \"spt-00035-79\" ; ns0:member &lt;https://data.vmm.be/id/loc-00019-33&gt;, [ sosa:madeBySensor &lt;urn:ngsi-v2:cot-imec-be:Device:imec-iow-UR5gEycRuaafxnhvjd9jnU&gt; ; ns1:result [ ns2:value [ ns3:value 2.043000e+1 ; ns3:unitCode &lt;https://data.vmm.be/id/CEL&gt; ] ] ; ns1:phenomenonTime \"2023-03-12T18:31:17.003Z\"^^xsd:datetime ; ns1:observedProperty &lt;https://data.vmm.be/concept/waterkwaliteitparameter/temperatuur&gt; ; ns1:featureOfInterest &lt;https://data.vmm.be/id/spt-00035-79&gt; ; a &lt;http://def.isotc211.org/iso19156/2011/Measurement#OM_Measurement&gt; ], [ sosa:madeBySensor &lt;urn:ngsi-v2:cot-imec-be:Device:imec-iow-UR5gEycRuaafxnhvjd9jnU&gt; ; ns1:result [ ns2:value [ ns3:value 1442 ; ns3:unitCode &lt;https://data.vmm.be/id/HP&gt; ] ] ; ns1:phenomenonTime \"2023-03-12T18:31:17.003Z\"^^xsd:datetime ; ns1:observedProperty &lt;https://data.vmm.be/concept/observatieparameter/hydrostatische-druk&gt; ; ns1:featureOfInterest &lt;https://data.vmm.be/id/spt-00035-79&gt; ; a &lt;http://def.isotc211.org/iso19156/2011/Measurement#OM_Measurement&gt; ], [ sosa:madeBySensor &lt;urn:ngsi-v2:cot-imec-be:Device:imec-iow-UR5gEycRuaafxnhvjd9jnU&gt; ; ns1:result [ ns2:value [ ns3:value 6150 ; ns3:unitCode &lt;https://data.vmm.be/id/G42&gt; ] ] ; ns1:phenomenonTime \"2023-03-12T18:31:17.003Z\"^^xsd:datetime ; ns1:observedProperty &lt;https://data.vmm.be/concept/waterkwaliteitparameter/conductiviteit&gt; ; ns1:featureOfInterest &lt;https://data.vmm.be/id/spt-00035-79&gt; ; a &lt;http://def.isotc211.org/iso19156/2011/Measurement#OM_Measurement&gt; ] ; a &lt;https://data.vlaanderen.be/ns/mobiliteit#ObservationCollection&gt; . &lt;https://data.vmm.be/id/loc-00019-33&gt; a &lt;http://def.isotc211.org/iso19156/2011/SpatialSamplingFeature#SF_SpatialSamplingFeature&gt; . | Run the following command to post the observation.ttl to the LDES Server. curl -X POST http://localhost:8080/observations -H \"Content-Type: application/ttl\" -d \"@observation.ttl\" . | . Replicating an LDES using the LDES Client . | Create a ldio.yml file in the same directory as your docker-compose.yml with the following content: . orchestrator: pipelines: - name: gipod-replicator description: \"HTTP polling, OSLO transformation, version creation &amp; HTTP sending.\" input: name: be.vlaanderen.informatievlaanderen.ldes.ldi.client.LdioLdesClient config: url: https://private-api.gipod.vlaanderen.be/api/v1/ldes/mobility-hindrances outputs: - name: be.vlaanderen.informatievlaanderen.ldes.ldio.LdioHttpOut config: endpoint: http://host.docker.internal:8080/mobility-hindrances content-type: application/n-quads . | Execute the following command to start up the LDIO docker compose up ldio-workbench -d . | Validate your LDES server is being populated by going to http://localhost:8080/mobility-hindrances/by-page?pageNumber=1 and http://localhost:8080/mobility-hindrances/time-based. These streams should fill up as the LDES Client send members to your server. | . Tear down the infrastructure and remove the volumes . | Within the working directory, please run docker rm -f $(docker ps -a -q) | . ",
    "url": "/quickstart/quickstart#ldes-server--ldes-client",
    
    "relUrl": "/quickstart/quickstart#ldes-server--ldes-client"
  },"32": {
    "doc": "Quick start",
    "title": "LDES2Service",
    "content": "***Please Note!*** _The purpose of the quick start try-out is solely to create _[Pagination fragmentation](https://github.com/Informatievlaanderen/VSDS-LDESServer4J/tree/main/ldes-fragmentisers/ldes-fragmentisers-pagination)_ for the self-generated data type to _[LDES format](https://semiceu.github.io/LinkedDataEventStreams/)_. To support other fragments, data types, or other features, e.g., retention, caching, etc., please consult _[LDES Server Manual](https://github.com/Informatievlaanderen/VSDS-LDESServer4J)_ for the configurations._ [Vlaamse Smart Data Space](https://www.vlaanderen.be/digitaal-vlaanderen/onze-oplossingen/vlaamse-smart-data-space)_ projects also provide methods for transforming data to *[LDES format](https://semiceu.github.io/LinkedDataEventStreams/)_, e.g. from _[NGSI-V2](https://vloca-kennishub.vlaanderen.be/NGSI-v2)* to *[NGSI-LD](https://en.wikipedia.org/wiki/NGSI-LD)_, from NGSI to _[OSLO](https://www.vlaanderen.be/digitaal-vlaanderen/onze-oplossingen/oslo)_ Model, from _[NGSI-V2](https://vloca-kennishub.vlaanderen.be/NGSI-v2)_ to _[LDES (LinkedDataEventStreams)](https://semiceu.github.io/LinkedDataEventStreams/). Having trouble implementing this example? Please post your issue on the [VSDS Tech docs repo](https://github.com/Informatievlaanderen/VSDS-Tech-Docs/issues). In this short example below, we show you how the components of the VSDS can be used to link multiple data streams across domains and systems using LDES. The whole story can be read here: Real-Time Data Linkage via Linked Data Event Streams . Before starting . | Docker has been installed on your device. | Local Ports 8443 and 7200 are accessible. | . Start your LDES workbench . | Create a local docker-compose.yml file with the following content, or you could use the current good-to-go GetStarted GRAR project: | . version: ‘3’ volumes: nifi-database-repository: nifi-flowfile-repository: nifi-content-repository: nifi-provenance-repository: nifi-state: nifi-logs: nifi-nars: nifi-conf: networks: front-tier: driver: bridge back-tier: driver: bridge services: nifi: image: ghcr.io/informatievlaanderen/ldes-workbench-nifi:20230214T123440 #20221109T103745 container_name: nifi-graph restart: unless-stopped ports: # HTTPS - 8443:8443/tcp volumes: - ./nifi-extensions:/opt/nifi/nifi-current/extensions environment: - NIFI_UI_PORT=8443 - SINGLE_USER_CREDENTIALS_USERNAME=admin - SINGLE_USER_CREDENTIALS_PASSWORD=admin123456789 - NIFI_WORKFLOW_LISTEN_PORT=9005 - NIFI_JVM_HEAP_INIT=8g - NIFI_JVM_HEAP_MAX=8g graphdb: image: ontotext/graphdb:10.0.2 container_name: graphdb3 ports: # HTTP - 7200:7200 . To begin, the command for launching GraphDb and Apache NiFi within a docker container is as follows: . docker compose up --build . Apache Nifi runs on port 8443:8443/tcp :https://localhost:8443 GraphDB runs on port 7200 :7200/tcp : http://localhost:7200 . A data flow can be built up by dragging in building blocks in the graphical user interface of Apache NiFi. This docker container contains all the components out the latest version of the LDES workbench (e.g., latest LDES Client, LDES server, etc.). Every data stream can be transformed into a Linked Data Event Stream with a conversion data flow. Starting from a published LDES . In this example, we start from published LDES . @prefix conceptscheme: &lt;https://data.vlaanderen.be/id/conceptscheme/&gt; . @prefix generiek: &lt;https://data.vlaanderen.be/ns/generiek#&gt; . @prefix geosparql: &lt;http://www.opengis.net/ont/geosparql#&gt; . @prefix ldes: &lt;https://w3id.org/ldes#&gt; . @prefix locn: &lt;https://www.w3.org/ns/locn#&gt; . @prefix prov: &lt;http://www.w3.org/ns/prov#&gt; . @prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; . @prefix relation: &lt;http://www.iana.org/assignments/relation/&gt; . @prefix terms: &lt;http://purl.org/dc/terms/&gt; . @prefix tree: &lt;https://w3id.org/tree#&gt; . https://data.vlaanderen.be/id/adres/1658513 relation:self \"https://api.basisregisters.staging-vlaanderen.be/v2/adressen/1658513\" . [ https://data.vlaanderen.be/ns/gebouw#bestaatUit &lt;https://data.vlaanderen.be/id/gebouweenheid/17631816/2022-06-27T16:23:49+02:00&gt; ; generiek:lokaleIdentificator \"17631524\"] . [ https://data.vlaanderen.be/ns/gebouw#bestaatUit &lt;https://data.vlaanderen.be/id/gebouweenheid/6311633/2022-06-27T16:23:48+02:00&gt; ; generiek:lokaleIdentificator \"6310652\"] . https://data.vlaanderen.be/id/gebouweenheid/14539518/2022-06-27T16:23:32+02:00 rdf:type &lt;https://data.vlaanderen.be/ns/gebouw#Gebouweenheid&gt; ; terms:isVersionOf &lt;https://data.vlaanderen.be/id/gebouweenheid/14539518&gt; ; prov:generatedAtTime \"2022-06-27T16:23:32+02:00\"^^&lt;http://www.w3.org/2001/XMLSchema#dateTime&gt; ; &lt;https://data.vlaanderen.be/ns/gebouw#Gebouweenheid.adres&gt; &lt;https://data.vlaanderen.be/id/adres/5162364&gt; ; &lt;https://data.vlaanderen.be/ns/gebouw#Gebouweenheid.geometrie&gt; [ conceptscheme:geometriemethode conceptscheme:aangeduidDoorBeheerder ; locn:geometry [ geosparql:asGML \"&lt;gml:Point srsName=\\\"http://www.opengis.net/def/crs/EPSG/0/31370\\\" xmlns:gml=\\\"http://www.opengis.net/gml/3.2\\\"&gt;&lt;gml:pos&gt;200647.71 210393.46&lt;/gml:pos&gt;&lt;/gml:Point&gt;\"^^geosparql:gmlLiteral ] ] ; &lt;https://data.vlaanderen.be/ns/gebouw#Gebouweenheid.status&gt; conceptscheme:gerealiseerd ; &lt;https://data.vlaanderen.be/ns/gebouw#functie&gt; conceptscheme:nietGekend ; generiek:lokaleIdentificator \"14539518\" ; generiek:naamruimte \"https://data.vlaanderen.be/id/gebouweenheid\" ; generiek:versieIdentificator \"2022-06-27T16:23:32+02:00\" . &lt;https://data.vlaanderen.be/id/adres/2213034&gt; relation:self \"https://api.basisregisters.staging-vlaanderen.be/v2/adressen/2213034\" . Consuming an LDES . For this purpose, the three Linked Data Event streams are stored in a GraphDB to facilitate efficient and effective data consumption. GraphDB supports complex semantic queries and inference, making discovering meaningful relationships between different data sources possible. | Start Apache NiFi on localhost:8433 . | Import data flow by importing Apache NiFi configuration file. See this site for more info . | Add LDES endpoint in the LDES client . | Start Apache NiFi data flow . | . | Open GraphDB on localhost:7200/sparql . | Run a semantic query with SPARQL . | . Below you find an example of a SPARQL query. This query returns triples with information about the parcel LDES. PREFIX adres: &lt;https://data.vlaanderen.be/id/adres/&gt; PREFIX gebouw: &lt;https://data.vlaanderen.be/id/?gebouweenheid/&gt; PREFIX gebouweenheid: &lt;https://data.vlaanderen.be/ns/gebouw#&gt; PREFIX prov: &lt;http://www.w3.org/ns/prov#&gt; PREFIX generiek: &lt;https://data.vlaanderen.be/ns/generiek#&gt; PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; construct { ?gebouweenheid prov:generatedAtTime ?generatedAtTime . ?gebouweenheid rdf:type ?type .?gebouweenheid gebouweenheid:Gebouweenheid.geometrie ?geometrie . ?gebouweenheid gebouweenheid:Gebouweenheid.status ?status . ?gebouweenheid gebouweenheid:functie ?functie . ?gebouweenheid generiek:lokaleIdentificator ?lokaleIdentificator . ?gebouweenheid generiek:naamruimte ?naamruimte . ?gebouweenheid generiek:versieIdentificator ?versieIdentificator . } where { ?gebouweenheid &lt;https://data.vlaanderen.be/ns/gebouw#Gebouweenheid.adres&gt; &lt;https://data.vlaanderen.be/id/adres/1864311&gt; . OPTIONAL{ ?gebouweenheid prov:generatedAtTime ?generatedAtTime . ?gebouweenheid rdf:type ?type . ?gebouweenheid gebouweenheid:Gebouweenheid.geometrie ?geometrie . ?gebouweenheid gebouweenheid:Gebouweenheid.status ?status . ?gebouweenheid gebouweenheid:functie ?functie . ?gebouweenheid generiek:lokaleIdentificator ?lokaleIdentificator . ?gebouweenheid generiek:naamruimte ?naamruimte . ?gebouweenheid generiek:versieIdentificator ?versieIdentificator . }} . This SPARQL query returns multiple triples with information about building units. These triples can then be converted into an RDF file. The RDF file should be as follow: . @prefix ns1: &lt;https://data.vlaanderen.be/ns/generiek#&gt; . @prefix ns2: &lt;https://data.vlaanderen.be/ns/gebouw#&gt; . @prefix ns3: &lt;http://www.w3.org/ns/prov#&gt; . @prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; . &lt;https://data.vlaanderen.be/id/gebouweenheid/14635685&gt; a ns2:Gebouweenheid ; ns3:generatedAtTime \"2022-06-27T20:51:02+02:00\"^^xsd:dateTime ; ns2:Gebouweenheid.geometrie [ ] ; ns2:Gebouweenheid.status &lt;https://data.vlaanderen.be/id/conceptscheme/gerealiseerd&gt; ; ns2:functie &lt;https://data.vlaanderen.be/id/conceptscheme/nietGekend&gt; ; ns1:lokaleIdentificator \"14635685\" ; ns1:naamruimte \"https://data.vlaanderen.be/id/gebouweenheid\" ; ns1:versieIdentificator \"2022-06-27T20:51:02+02:00\" . ",
    "url": "/quickstart/quickstart#ldes2service",
    
    "relUrl": "/quickstart/quickstart#ldes2service"
  }
}
